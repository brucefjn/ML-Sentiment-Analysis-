{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis with Transformer Fine-Tuning\n",
        "\n",
        "This project builds a sentiment classification system for movie reviews.\n",
        "\n",
        "Originally, the project began with a classical machine learning approach\n",
        "(TF-IDF + Logistic Regression).\n",
        "\n",
        "However, to build a more rigorous and modern system capable of understanding\n",
        "grammar and contextual meaning, the model was upgraded to a Transformer-based\n",
        "architecture (DistilBERT).\n",
        "\n",
        "Final model: Fine-tuned DistilBERT for binary sentiment classification.\n"
      ],
      "metadata": {
        "id": "V4osN0Olrkxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "\n",
        "The dataset consists of labeled movie reviews:\n",
        "\n",
        "- `Review`: raw text input\n",
        "- `Emotion`: sentiment label (positive / negative)\n",
        "\n",
        "We perform supervised learning using these labeled examples.\n"
      ],
      "metadata": {
        "id": "kW_9mU_1rr_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O movie_reviews 'https://drive.google.com/uc?export=view&id=1kWs6yOYpdjVr-liLPs4PKIs1qSpIohzS'"
      ],
      "metadata": {
        "id": "AxhTOdcggFIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Label Data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "data = pd.read_csv('movie_reviews', delimiter=\",\")\n",
        "data.head(20)\n"
      ],
      "metadata": {
        "id": "ixvBBotEg9hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dataset Inspection\n",
        "\n",
        "We inspect:\n",
        "\n",
        "- Total dataset size\n",
        "- Class distribution\n",
        "- Label balance\n",
        "\n",
        "This ensures the dataset is suitable for supervised learning.\n"
      ],
      "metadata": {
        "id": "QmOMnBImsG-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = data['Review']\n",
        "y = data['Emotion']"
      ],
      "metadata": {
        "id": "GkrlLo1NiPCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install transformers datasets accelerate torch scikit-learn\n"
      ],
      "metadata": {
        "id": "oQ8-VFesijIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "NowAFjRSc4A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Label Encoding\n",
        "\n",
        "Sentiment labels are mapped to numeric IDs to allow\n",
        "training using cross-entropy loss.\n"
      ],
      "metadata": {
        "id": "GFe7_5QVsffB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[[\"Review\", \"Emotion\"]].dropna()\n",
        "\n",
        "# Map labels to ids\n",
        "labels = sorted(data[\"Emotion\"].unique())\n",
        "label2id = {l: i for i, l in enumerate(labels)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "data[\"label\"] = data[\"Emotion\"].map(label2id)\n",
        "\n",
        "print(\"Labels:\", labels)\n",
        "print(data[\"Emotion\"].value_counts())\n",
        "print(\"Total rows:\", len(data))\n",
        "\n"
      ],
      "metadata": {
        "id": "ETnsOyqThzdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train-Test Split\n",
        "\n",
        "The dataset is split into:\n",
        "\n",
        "- 80% training data\n",
        "- 20% testing data\n",
        "\n",
        "Stratified sampling preserves class proportions.\n"
      ],
      "metadata": {
        "id": "cXmJUf4Bsits"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(\n",
        "    data,\n",
        "    test_size=0.2,\n",
        "    random_state=20,\n",
        "    stratify=data[\"label\"]\n",
        ")\n",
        "\n",
        "print(\"Train:\", len(train_df), \"Test:\", len(test_df))\n",
        "\n"
      ],
      "metadata": {
        "id": "i4xWUqA1jNPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = Dataset.from_pandas(train_df[[\"Review\", \"label\"]])\n",
        "test_ds  = Dataset.from_pandas(test_df[[\"Review\", \"label\"]])"
      ],
      "metadata": {
        "id": "nJ1Z8DgJk15K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Tokenization\n",
        "\n",
        "Raw text is converted into token IDs using a pretrained\n",
        "DistilBERT tokenizer.\n",
        "\n",
        "This enables contextual embedding and grammar-aware modeling.\n"
      ],
      "metadata": {
        "id": "KAV7ennWsruQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"Review\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "train_ds = train_ds.map(tokenize, batched=True)\n",
        "test_ds  = test_ds.map(tokenize, batched=True)\n",
        "\n",
        "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "train_ds.set_format(type=\"torch\", columns=cols)\n",
        "test_ds.set_format(type=\"torch\", columns=cols)\n"
      ],
      "metadata": {
        "id": "rdGiXMApYNT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Model Architecture\n",
        "\n",
        "We fine-tune DistilBERT, a Transformer-based neural network.\n",
        "\n",
        "Architecture:\n",
        "\n",
        "- Pretrained DistilBERT encoder\n",
        "- Linear classification head\n",
        "- Softmax output layer\n",
        "\n",
        "Training objective: minimize cross-entropy loss.\n"
      ],
      "metadata": {
        "id": "m63p190Osw5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, y_true = eval_pred\n",
        "    y_pred = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\")\n",
        "    }\n"
      ],
      "metadata": {
        "id": "US9Gy27tdsF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, inspect\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "print(\"transformers version:\", transformers.__version__)\n",
        "print(\"transformers file:\", transformers.__file__)\n",
        "print(\"TrainingArguments init params contains evaluation_strategy?\",\n",
        "      \"evaluation_strategy\" in str(inspect.signature(TrainingArguments.__init__)))\n",
        "print(\"TrainingArguments init params contains eval_strategy?\",\n",
        "      \"eval_strategy\" in str(inspect.signature(TrainingArguments.__init__)))\n"
      ],
      "metadata": {
        "id": "UJlYH566eoPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Training Configuration\n",
        "\n",
        "Hyperparameters:\n",
        "\n",
        "- Learning rate: 2e-5\n",
        "- Batch size: 16\n",
        "- Epochs: 2\n",
        "- Optimizer: AdamW\n",
        "- Metric: F1 Macro\n",
        "\n",
        "GPU acceleration is used for efficient fine-tuning.\n"
      ],
      "metadata": {
        "id": "Q8edcykJs1ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"sentiment_bert\",\n",
        "    eval_strategy=\"epoch\",     # <-- FIXED\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1_macro\",\n",
        "    report_to=\"none\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "JgZPToCUdzb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding, Trainer\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "qrhJqMyOfi63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Model Training & Evaluation\n",
        "\n",
        "The pretrained transformer is fine-tuned\n",
        "on the labeled movie review dataset.\n",
        "\n",
        "Final performance metrics:\n",
        "\n",
        "- Accuracy ≈ 91%\n",
        "- F1 Macro ≈ 91%\n",
        "\n",
        "The model demonstrates strong generalization performance.\n"
      ],
      "metadata": {
        "id": "P6TSn0t4s61t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "gnJ9pQ1fglMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 9. Example of Prediction"
      ],
      "metadata": {
        "id": "GjyitfBUtdin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_with_proba(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    preds = probs.argmax(axis=1)\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        pred_label = id2label[int(preds[i])]\n",
        "        prob_dict = {id2label[j]: float(probs[i][j]) for j in range(len(labels))}\n",
        "        print(\"\\nTEXT:\", text)\n",
        "        print(\"PRED:\", pred_label)\n",
        "        print(\"PROBS:\", prob_dict)\n",
        "\n",
        "predict_with_proba([\n",
        "    \"This movie is not as good.\",\n",
        "    \"Absolutely fantastic — I loved it.\"\n",
        "])\n"
      ],
      "metadata": {
        "id": "EWGeNJKLfADD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Interactive Prediction Demo\n",
        "\n",
        "Users can input custom movie reviews\n",
        "to observe real-time sentiment predictions\n",
        "and confidence scores.\n"
      ],
      "metadata": {
        "id": "yRlV8KF_tZ2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def predict_texts(texts):\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=256\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "    for text, p in zip(texts, probs):\n",
        "        pred_id = int(p.argmax())\n",
        "        pred_label = id2label[pred_id]\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"TEXT:\", text)\n",
        "        print(\"PREDICTION:\", pred_label)\n",
        "        print(\"CONFIDENCE:\", round(float(p[pred_id]), 4))\n",
        "        print(\"ALL PROBS:\")\n",
        "        for i in range(len(p)):\n",
        "            print(f\"  {id2label[i]}: {round(float(p[i]), 4)}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Feel free edit this line ↓↓↓\n",
        "# -----------------------------\n",
        "\n",
        "user_input = input(\"Enter a movie review sentence: \")\n",
        "\n",
        "predict_texts(user_input)\n"
      ],
      "metadata": {
        "id": "aVFAYSNkp5HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Error Analysis\n",
        "\n",
        "We examine misclassified examples to understand\n",
        "model limitations.\n",
        "\n",
        "Observed challenges:\n",
        "\n",
        "- Comparative sentiment\n",
        "- Sarcasm\n",
        "- Pragmatic reversal\n",
        "- Contrastive structures\n"
      ],
      "metadata": {
        "id": "4_NHqPDZuio2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_texts([\n",
        "    \"I love it.\",\n",
        "    \"I love it. Not.\",\n",
        "    \"Great movie... said no one ever.\",\n",
        "    \"This was amazing... until it wasn't.\",\n",
        "    \"This movie is not as good as Titanic; it's already good enough.\",\n",
        "    \"I thought it was good; it was actually terrible.\"\n",
        "])"
      ],
      "metadata": {
        "id": "u2G6_e8quFB7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}